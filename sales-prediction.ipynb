{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debe0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def definitive_clean_and_aggregate_data(raw_data_path, output_path):\n",
    "    \"\"\"\n",
    "    This function carefully cleans the data. It fills missing names from account_id,\n",
    "    standardizes KPI names, removes useless KPIs, and aggregates the data\n",
    "    by summing both yearly and monthly values.\n",
    "\n",
    "    Args:\n",
    "        raw_data_path (str): Path to the raw data CSV file.\n",
    "        output_path (str): Path to save the cleaned master CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The clean and prepared DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"--- Final Phase 1: Data Foundation (Correct Aggregation) ---\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        df = pd.read_csv(raw_data_path)\n",
    "        print(f\"Successfully loaded raw data from '{raw_data_path}'. Shape: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{raw_data_path}' not found.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Correct Data Types\n",
    "    print(\"\\nStep 1: Correcting data types...\")\n",
    "    df['monthly_value'] = pd.to_numeric(df['monthly_value'], errors='coerce').fillna(0)\n",
    "    df['yearly_value'] = pd.to_numeric(df['yearly_value'], errors='coerce').fillna(0)\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    print(\"Data types corrected.\")\n",
    "\n",
    "    # 3. Intelligently Fill Missing KPI Names\n",
    "    print(\"\\nStep 2: Carefully filling missing KPI names...\")\n",
    "    name_map = df.dropna(subset=['english_name']).groupby('account_id')['english_name'].first().to_dict()\n",
    "    df['english_name'] = df['english_name'].fillna(df['account_id'].map(name_map))\n",
    "    df['english_name'].fillna('Unknown KPI', inplace=True)\n",
    "    print(\"Missing names filled based on account_id.\")\n",
    "\n",
    "    # 4. Deep Clean and Normalize All KPI Names\n",
    "    print(\"\\nStep 3: Performing deep cleaning on KPI names...\")\n",
    "    df['english_name_cleaned'] = df['english_name'].astype(str).str.strip().str.upper()\n",
    "    df['english_name_cleaned'] = df['english_name_cleaned'].apply(lambda x: re.sub(r'[^A-Z0-9\\s-]', '', x).strip())\n",
    "    df['english_name_cleaned'] = df['english_name_cleaned'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "    print(\"KPI names have been standardized.\")\n",
    "\n",
    "    # 5. Remove Useless and Invalid KPIs\n",
    "    print(\"\\nStep 4: Removing useless placeholder KPIs...\")\n",
    "    original_rows = len(df)\n",
    "    df = df[~df['english_name_cleaned'].isin(['-', ''])]\n",
    "    rows_removed = original_rows - len(df)\n",
    "    print(f\"{rows_removed} rows with placeholder KPI names have been removed.\")\n",
    "\n",
    "    # 6. Aggregate by summing monthly and yearly values\n",
    "    print(\"\\nStep 5: Aggregating data based on cleaned KPI names...\")\n",
    "    df_aggregated = df.groupby(['english_name_cleaned', 'year', 'month']).agg({\n",
    "        'monthly_value': 'sum',\n",
    "        'yearly_value': 'sum'  # This correctly preserves the yearly total from duplicates\n",
    "    }).reset_index()\n",
    "    print(f\"Data has been aggregated. Original rows: {len(df)}, Aggregated rows: {len(df_aggregated)}\")\n",
    "\n",
    "    # 7. Confirm Final Number of Unique KPIs\n",
    "    unique_kpi_count = df_aggregated['english_name_cleaned'].nunique()\n",
    "    print(f\"\\nCONFIRMED: Found {unique_kpi_count} unique, valid KPIs after deep cleaning.\")\n",
    "\n",
    "    # 8. Create Date Column and Finalize Structure\n",
    "    print(\"\\nStep 6: Finalizing the dataset...\")\n",
    "    df_aggregated['date'] = pd.to_datetime(df_aggregated[['year', 'month']].assign(day=1))\n",
    "    df_aggregated.rename(columns={'english_name_cleaned': 'english_name'}, inplace=True)\n",
    "    final_df = df_aggregated[['date', 'english_name', 'monthly_value', 'yearly_value', 'year', 'month']]\n",
    "    final_df.sort_values(by=['english_name', 'date'], inplace=True)\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Dataset has been finalized and sorted.\")\n",
    "\n",
    "    # 9. Save the Clean Master File\n",
    "    try:\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n--- Phase 1 Complete ---\")\n",
    "        print(f\"Final clean master data file has been successfully saved to '{output_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return None\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define file paths\n",
    "    raw_file = 'FS-data-80475.csv'\n",
    "    output_file = 'cleaned_master_data.csv'\n",
    "    \n",
    "    # Execute the final cleaning process\n",
    "    cleaned_data = definitive_clean_and_aggregate_data(raw_file, output_file)\n",
    "    \n",
    "    if cleaned_data is not None:\n",
    "        print(\"\\n--- First 5 rows of the new, final clean master data: ---\")\n",
    "        print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae31fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def perform_exploratory_analysis(cleaned_data_path):\n",
    "    \"\"\"\n",
    "    Performs exploratory data analysis on the cleaned and aggregated master data.\n",
    "\n",
    "    Args:\n",
    "        cleaned_data_path (str): The path to the cleaned_master_data.csv file.\n",
    "    \"\"\"\n",
    "    print(\"--- Phase 2: Exploratory Data Analysis ---\")\n",
    "    \n",
    "    # 1. Load the Clean Data\n",
    "    try:\n",
    "        df = pd.read_csv(cleaned_data_path, parse_dates=['date'])\n",
    "        print(f\"Successfully loaded clean master data from '{cleaned_data_path}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{cleaned_data_path}' was not found. Please run Phase 1 first.\")\n",
    "        return\n",
    "\n",
    "    # Set plot style for better aesthetics\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (15, 7)\n",
    "\n",
    "    # 2. Identify and Visualize a Key Business KPI\n",
    "    print(\"\\nStep 1: Visualizing a key business metric...\")\n",
    "    \n",
    "    # We'll focus on 'TOTAL GROSS PROFIT' as an example. You can change this name.\n",
    "    kpi_to_plot_name = 'TOTAL GROSS PROFIT' \n",
    "    \n",
    "    kpi_data = df[df['english_name'] == kpi_to_plot_name]\n",
    "\n",
    "    if kpi_data.empty:\n",
    "        print(f\"Warning: Could not find data for KPI named '{kpi_to_plot_name}'. Please check the name.\")\n",
    "    else:\n",
    "        plt.figure()\n",
    "        plt.plot(kpi_data['date'], kpi_data['monthly_value'], marker='o', linestyle='-', color='#0068C9')\n",
    "        plt.title(f'Monthly Trend for: {kpi_to_plot_name}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Monthly Value')\n",
    "        plt.show()\n",
    "\n",
    "    # 3. Correlation Analysis for Top KPIs\n",
    "    print(\"\\nStep 2: Analyzing correlation between top KPIs...\")\n",
    "    \n",
    "    # Find the top 20 KPIs by average monthly value to keep the heatmap readable\n",
    "    kpi_summary = df.groupby('english_name')['monthly_value'].mean().reset_index()\n",
    "    top_20_kpis = kpi_summary.sort_values(by='monthly_value', ascending=False).head(20)['english_name'].tolist()\n",
    "\n",
    "    # Pivot the data to a 'wide' format for correlation\n",
    "    df_wide = df[df['english_name'].isin(top_20_kpis)].pivot_table(\n",
    "        index='date', \n",
    "        columns='english_name', \n",
    "        values='monthly_value'\n",
    "    )\n",
    "    \n",
    "    # Fill any missing values before calculating correlation\n",
    "    df_wide.ffill(inplace=True)\n",
    "    df_wide.bfill(inplace=True)\n",
    "\n",
    "    # Calculate and plot the correlation matrix\n",
    "    correlation_matrix = df_wide.corr()\n",
    "\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
    "    plt.title('Correlation Matrix of Top 20 KPIs')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n--- Phase 2 Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define the input file path\n",
    "    cleaned_file = 'cleaned_master_data.csv'\n",
    "    \n",
    "    # Execute the analysis\n",
    "    perform_exploratory_analysis(cleaned_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a819dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import stopit\n",
    "import os\n",
    "\n",
    "# Suppress informational messages from Prophet\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def generate_all_forecasts(cleaned_data_path, output_path, months_to_predict=3, timeout_seconds=45):\n",
    "    \"\"\"\n",
    "    Trains a Prophet model for each unique KPI from the clean master data file.\n",
    "    This is a fresh, single run with robust error handling.\n",
    "\n",
    "    Args:\n",
    "        cleaned_data_path (str): Path to the cleaned_master_data.csv file.\n",
    "        output_path (str): Path to save the final forecast data.\n",
    "        months_to_predict (int): The number of months to forecast.\n",
    "        timeout_seconds (int): Max seconds to wait for a model to train.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing all successful forecasts.\n",
    "    \"\"\"\n",
    "    print(\"--- Phase 3: Predictive Modeling ---\")\n",
    "    \n",
    "    # 1. Load the Clean Data\n",
    "    try:\n",
    "        df = pd.read_csv(cleaned_data_path, parse_dates=['date'])\n",
    "        print(f\"Successfully loaded clean master data from '{cleaned_data_path}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{cleaned_data_path}' was not found. Please run Phase 1 first.\")\n",
    "        return None\n",
    "\n",
    "    # Start with a fresh forecast file\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Removing old forecast file '{output_path}' to start fresh.\")\n",
    "        os.remove(output_path)\n",
    "\n",
    "    all_forecasts = []\n",
    "    unique_kpis = df['english_name'].unique()\n",
    "    total_kpis = len(unique_kpis)\n",
    "    print(f\"\\nFound {total_kpis} unique KPIs to forecast.\")\n",
    "\n",
    "    # 2. Loop Through Each KPI to Train and Forecast\n",
    "    for i, kpi_name in enumerate(unique_kpis):\n",
    "        print(f\"Processing KPI {i+1}/{total_kpis}: {kpi_name}\")\n",
    "        \n",
    "        try:\n",
    "            kpi_data = df[df['english_name'] == kpi_name].copy()\n",
    "            \n",
    "            # Prophet requires specific column names: 'ds' for date and 'y' for value\n",
    "            kpi_data.rename(columns={'date': 'ds', 'monthly_value': 'y'}, inplace=True)\n",
    "            \n",
    "            # --- Data Validation ---\n",
    "            if len(kpi_data) < 12 or kpi_data['y'].nunique() < 2:\n",
    "                print(f\"  -> Skipping '{kpi_name}' due to insufficient data or constant value.\")\n",
    "                continue\n",
    "\n",
    "            # --- Normalization ---\n",
    "            y_values = kpi_data['y'].values.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            kpi_data['y_scaled'] = scaler.fit_transform(y_values)\n",
    "\n",
    "            # --- Model Training with Timeout ---\n",
    "            model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "            \n",
    "            print(f\"  -> Fitting model...\")\n",
    "            with stopit.ThreadingTimeout(timeout_seconds) as context_manager:\n",
    "                model.fit(kpi_data[['ds', 'y_scaled']].rename(columns={'y_scaled': 'y'}))\n",
    "            \n",
    "            if context_manager.state != stopit.ThreadingTimeout.EXECUTED:\n",
    "                print(f\"  -> TIMEOUT: Model fitting for '{kpi_name}' took too long. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  -> Model fitting complete.\")\n",
    "            \n",
    "            # --- Prediction and Rescaling ---\n",
    "            future_dates = model.make_future_dataframe(periods=months_to_predict, freq='MS')\n",
    "            forecast = model.predict(future_dates)\n",
    "            \n",
    "            predicted_values_scaled = forecast[['yhat', 'yhat_lower', 'yhat_upper']].values\n",
    "            forecast[['yhat', 'yhat_lower', 'yhat_upper']] = scaler.inverse_transform(predicted_values_scaled)\n",
    "\n",
    "            # --- Store Results ---\n",
    "            forecast['english_name'] = kpi_name\n",
    "            results = forecast[['ds', 'english_name', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "            results = results.merge(kpi_data[['ds', 'y']], on='ds', how='left')\n",
    "            \n",
    "            all_forecasts.append(results)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  -> FAILED to process '{kpi_name}'. Error: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not all_forecasts:\n",
    "        print(\"\\nNo forecasts could be generated.\")\n",
    "        return None\n",
    "\n",
    "    # 3. Combine and Save Final Forecast File\n",
    "    final_forecast_df = pd.concat(all_forecasts)\n",
    "    final_forecast_df.rename(columns={\n",
    "        'ds': 'date', \n",
    "        'y': 'actual_value', \n",
    "        'yhat': 'predicted_value'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    try:\n",
    "        final_forecast_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n--- Phase 3 Complete ---\")\n",
    "        print(f\"Forecast master data file saved successfully to '{output_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return None\n",
    "\n",
    "    return final_forecast_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define file paths\n",
    "    cleaned_file = 'cleaned_master_data.csv'\n",
    "    output_file = 'forecast_master_data.csv'\n",
    "    \n",
    "    # Execute the forecasting process\n",
    "    forecast_data = generate_all_forecasts(cleaned_file, output_file)\n",
    "    \n",
    "    if forecast_data is not None:\n",
    "        print(\"\\n--- First 5 rows of the new forecast master data: ---\")\n",
    "        print(forecast_data.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
